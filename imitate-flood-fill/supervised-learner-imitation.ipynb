{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create Hungry Geese agent by supervised learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this Notebook is to provide an easy way to participate in this competition, so that many people can enjoy Hungry Geese. Kagglers are familiar with supervised learning and keras is one of the easiest frameworks to use. So, I would like to introduce a simple approach using keras with supervised learning. Of course, if you are aiming for a higher level, you will need reinforcement learning. I'm learning RL through this competition. Let's enjoy Hungry Geese together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on a lot of public notebooks. In particular, the following ones:  \n",
    "Most of technical parts including model architecture, data pipeline etc: https://www.kaggle.com/yuricat/smart-geese-trained-by-reinforcement-learning  \n",
    "Scraping episode files: https://www.kaggle.com/robga/simulations-episode-scraper-match-downloader  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "import bz2\n",
    "import base64\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score, recall_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import (GlobalAveragePooling2D, Dense, Conv2D, Activation, Lambda, Add, \n",
    "                                     BatchNormalization, Input, Lambda)\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l1_l2 ,l1, l2\n",
    "from tensorflow.keras import mixed_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radabelief Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sohai\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/ben-arnao/Radabelief\n",
    "import tensorflow as tf\n",
    "from tensorflow_addons.utils.types import FloatTensorLike\n",
    "\n",
    "from typing import Union, Callable, Dict\n",
    "from typeguard import typechecked\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
    "class RadaBelief(tf.keras.optimizers.Optimizer):\n",
    "\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "            self,\n",
    "            learning_rate: Union[FloatTensorLike, Callable, Dict] = 1e-4,\n",
    "            beta_1: FloatTensorLike = 0.9,\n",
    "            beta_2: FloatTensorLike = 0.999,\n",
    "            epsilon: FloatTensorLike = 1e-12,\n",
    "            warmup_steps: int = 10000,\n",
    "            name: str = \"Radabelief\",\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(name, **kwargs)\n",
    "\n",
    "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n",
    "        self._set_hyper(\"beta_1\", beta_1)\n",
    "        self._set_hyper(\"beta_2\", beta_2)\n",
    "        self._set_hyper(\"warmup_steps\", warmup_steps)\n",
    "        self.epsilon = epsilon or tf.keras.backend.epsilon()\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"m\")\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"v\")\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        params = self.weights\n",
    "        num_vars = int((len(params) - 1) / 2)\n",
    "        if len(weights) == 3 * num_vars + 1:\n",
    "            weights = weights[: len(params)]\n",
    "        super().set_weights(weights)\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "\n",
    "        # *current* learn rate of optimizer (changed by ReduceLROnPlateau for example)\n",
    "        lr_t = self.lr\n",
    "\n",
    "        # get previous moments\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        v = self.get_slot(var, \"v\")\n",
    "\n",
    "        # static hyperparameters of optimizer\n",
    "        lr_0 = self._get_hyper(\"learning_rate\", var_dtype)\n",
    "        warmup_steps = self._get_hyper(\"warmup_steps\", var_dtype)\n",
    "        beta_1 = self._get_hyper(\"beta_1\", var_dtype)\n",
    "        beta_2 = self._get_hyper(\"beta_2\", var_dtype)\n",
    "\n",
    "        # smaller epsilon == more bias == more like SGD\n",
    "        # larger epsilon == more adaptive, potential for large LR difference between variables\n",
    "        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n",
    "\n",
    "        # current step of optimizer\n",
    "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
    "\n",
    "        # use linearly scaled lr from 0 to initial LR while steps under 'warmup_steps'\n",
    "        lr = tf.where(\n",
    "            local_step <= warmup_steps,\n",
    "            (local_step / warmup_steps) * lr_0,\n",
    "            lr_t,\n",
    "        )\n",
    "\n",
    "        # calculate first moment of gradient (momemtum)\n",
    "        m_t = m.assign(\n",
    "            beta_1 * m + (1.0 - beta_1) * grad,\n",
    "            use_locking=self._use_locking\n",
    "        )\n",
    "\n",
    "        # calculate second moment of gradient (RMSprop)\n",
    "        # use 'tf.square(grad - m_t)' for Adabelief instead of 'tf.square(grad)'\n",
    "        v_t = v.assign(\n",
    "            beta_2 * v + (1.0 - beta_2) * tf.square(grad - m_t),\n",
    "            use_locking=self._use_locking,\n",
    "        )\n",
    "\n",
    "        # correct bias (mostly affects initial steps)\n",
    "        m_corr_t = m_t / (1.0 - tf.pow(beta_1, local_step))\n",
    "        v_corr_t = v_t / (1.0 - tf.pow(beta_2, local_step))\n",
    "\n",
    "        # calculate step\n",
    "        var_t = m_corr_t / (tf.sqrt(v_corr_t) + epsilon_t)\n",
    "\n",
    "        # apply learn rate\n",
    "        var_update = var.assign_sub(lr * var_t,\n",
    "                                    use_locking=self._use_locking)\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        return tf.group(*updates)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "                \"beta_1\": self._serialize_hyperparameter(\"beta_1\"),\n",
    "                \"beta_2\": self._serialize_hyperparameter(\"beta_2\"),\n",
    "                \"epsilon\": self.epsilon,\n",
    "                \"warmup_steps\": self._serialize_hyperparameter(\"warmup_steps\"),\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Anealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "    \"\"\"Cosine annealing scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# besed on https://github.com/yu4u/mixup-generator\n",
    "class GeeseDataGenerator(Sequence):\n",
    "        'Generates data for Keras'\n",
    "        def __init__(self, X_train, y_train, batch_size=128, shuffle=True, sample_weight=None):\n",
    "            'Initialization'\n",
    "            self.batch_size = batch_size\n",
    "            self.X_train = X_train\n",
    "            self.y_train = y_train\n",
    "            self.shuffle = shuffle\n",
    "            self.on_epoch_end()\n",
    "            self.sample_weight = sample_weight\n",
    "    \n",
    "        def __len__(self):\n",
    "            'Denotes the number of batches per epoch'\n",
    "            return int(np.ceil(len(self.X_train) / self.batch_size))\n",
    "    \n",
    "        def __getitem__(self, index):\n",
    "            'Generate one batch of data'\n",
    "            # Generate indexes of the batch\n",
    "            indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "            # Generate data\n",
    "            if self.sample_weight is not None:\n",
    "                X, y, w = self.__data_generation(indexes)\n",
    "                return X, y, w\n",
    "            else:\n",
    "                X, y = self.__data_generation(indexes)\n",
    "                return X, y\n",
    "    \n",
    "        def on_epoch_end(self):\n",
    "            'Updates indexes after each epoch'\n",
    "            self.indexes = np.arange(len(self.X_train))\n",
    "            if self.shuffle == True:\n",
    "                np.random.shuffle(self.indexes)\n",
    "        \n",
    "        def __data_generation(self, batch_ids):\n",
    "            #_, h, w, c = self.X_train.shape\n",
    "        \n",
    "            X = self.X_train[batch_ids]\n",
    "            y = self.y_train[batch_ids]\n",
    "            if self.sample_weight is not None:\n",
    "                w = self.sample_weight[batch_ids]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            series_len = len(X)\n",
    "            \n",
    "            # vertical flip\n",
    "            if np.random.uniform(0,1)>0.5:\n",
    "                X = np.flip(X, axis=2)\n",
    "                y = y[:,[0,1,3,2]]\n",
    "                \n",
    "            # horizontal flip\n",
    "            if np.random.uniform(0,1)>0.5:\n",
    "                X = np.flip(X, axis=1)\n",
    "                y = y[:,[1,0,2,3]] \n",
    "            \n",
    "            if self.sample_weight is not None:\n",
    "                return X, y, w\n",
    "            else:\n",
    "                return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def create_dataset_from_json(filepath, json_object=None, standing=0):\n",
    "    if json_object is None:\n",
    "        json_open = open(path, 'r')\n",
    "        json_load = json.load(json_open)\n",
    "    else:\n",
    "        json_load = json_object\n",
    "        \n",
    "    try:\n",
    "        winner_index = np.argmax(np.argsort(json_load['rewards']) == 3-standing)\n",
    "\n",
    "        obses = []\n",
    "        X = []\n",
    "        y = []\n",
    "        actions = {'NORTH':0, 'SOUTH':1, 'WEST':2, 'EAST':3}\n",
    "\n",
    "        for i in range(len(json_load['steps'])-1):\n",
    "            if json_load['steps'][i][winner_index]['status'] == 'ACTIVE':\n",
    "                y_= json_load['steps'][i+1][winner_index]['action']\n",
    "                if y_ is not None:\n",
    "                    step = json_load['steps'][i]\n",
    "                    step[winner_index]['observation']['geese'] = step[0]['observation']['geese']\n",
    "                    step[winner_index]['observation']['food'] = step[0]['observation']['food']\n",
    "                    obses.append(step[winner_index]['observation'])\n",
    "                    y.append(actions[y_])        \n",
    "\n",
    "        for j in range(len(obses)):\n",
    "            X_ = make_input(obses[:j+1])\n",
    "            X_ = centerize(X_)\n",
    "            X.append(X_)\n",
    "\n",
    "        X = np.array(X, dtype=np.float32)#[starting_step:]\n",
    "        y = np.array(y, dtype=np.uint8)#[starting_step:]\n",
    "\n",
    "        return X, y\n",
    "    except:\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# based on https://www.kaggle.com/yuricat/smart-geese-trained-by-reinforcement-learning\n",
    "def centerize(b):\n",
    "    dy, dx = np.where(b[0])\n",
    "    centerize_y = (np.arange(0,7)-3+dy[0])%7\n",
    "    centerize_x = (np.arange(0,11)-5+dx[0])%11\n",
    "    \n",
    "    b = b[:, centerize_y,:]\n",
    "    b = b[:, :,centerize_x]\n",
    "    \n",
    "    return b\n",
    "\n",
    "def make_input(obses):\n",
    "    b = np.zeros((17, 7 * 11), dtype=np.float32)\n",
    "    obs = obses[-1]\n",
    "\n",
    "    for p, pos_list in enumerate(obs['geese']):\n",
    "        # head position\n",
    "        for pos in pos_list[:1]:\n",
    "            b[0 + (p - obs['index']) % 4, pos] = 1\n",
    "        # tip position\n",
    "        for pos in pos_list[-1:]:\n",
    "            b[4 + (p - obs['index']) % 4, pos] = 1\n",
    "        # whole position\n",
    "        for pos in pos_list:\n",
    "            b[8 + (p - obs['index']) % 4, pos] = 1\n",
    "            \n",
    "    # previous head position\n",
    "    if len(obses) > 1:\n",
    "        obs_prev = obses[-2]\n",
    "        for p, pos_list in enumerate(obs_prev['geese']):\n",
    "            for pos in pos_list[:1]:\n",
    "                b[12 + (p - obs['index']) % 4, pos] = 1\n",
    "\n",
    "    # food\n",
    "    for pos in obs['food']:\n",
    "        b[16, pos] = 1\n",
    "        \n",
    "    b = b.reshape(-1, 7, 11)\n",
    "    b = centerize(b) # Where to place the head is arbiterary dicision.\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# based on https://www.kaggle.com/yuricat/smart-geese-trained-by-reinforcement-learning\n",
    "def TorusConv2D(x, ch, kernel, padding=\"same\", strides=1, weight_decay=2e-3):\n",
    "    x = Lambda(lambda x: K.tile(x, n=(1,3,3,1)), \n",
    "               output_shape=lambda input_shape: (None, 3*input_shape[1], 3*input_shape[2], input_shape[3]))(x)\n",
    "    \n",
    "    x = Conv2D(ch, kernel, padding=padding, strides=strides,\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    \n",
    "    x = Lambda(lambda x: x[:,int(x.shape[1]/3):2*int(x.shape[1]/3), int(x.shape[2]/3):2*int(x.shape[2]/3),:], \n",
    "               output_shape=lambda input_shape: (None, int(input_shape[1]/3), int(input_shape[2]/3), input_shape[3]))(x)\n",
    "    return x\n",
    "\n",
    "def conv_bn_relu(x0, ch, kernel, padding=\"same\", strides=1, weight_decay=2e-3, add=False):\n",
    "    x = TorusConv2D(x0, ch, kernel, padding=padding, strides=strides,\n",
    "                      weight_decay=weight_decay)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    if add:\n",
    "        x = Add()([x0, x])\n",
    "    return x\n",
    "\n",
    "def GeeseNet(input_shape=(7, 11, 17), layers=12, filters=32, weight_decay=2e-3, compile=True):\n",
    "    input = Input(input_shape)\n",
    "    x = conv_bn_relu(input, filters, 3)\n",
    "    \n",
    "    for i in range(layers):\n",
    "        x = conv_bn_relu(x, filters, 3, add=True)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    output = Dense(4, activation='softmax', kernel_regularizer=l1_l2(l1=0.0005, l2=0.0005))(x)   \n",
    "    model = Model(input, output)\n",
    "    if compile:\n",
    "        model.compile(optimizer=RadaBelief(learning_rate=1e-3, epsilon=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "# use mixed precision\n",
    "policy = mixed_precision.Policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:12: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\sohai\\AppData\\Local\\Temp\\ipykernel_28360\\458220406.py:12: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if X is not 0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef590ebb8ba47fc8e2fb15c877dd4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m         X_train\u001b[38;5;241m.\u001b[39mappend(X)\n\u001b[0;32m     14\u001b[0m         y_train\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[1;32m---> 16\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(y_train)\n\u001b[0;32m     19\u001b[0m X_train, unique_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(X_train, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, return_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# Create data set from json\n",
    "# Only a few episodes are used to train here. Try it with your own json collection.\n",
    "paths = [path for path in glob(\"./flood_fill_3k_data/*.json\") if 'info' not in path]\n",
    "\n",
    "print(len(paths))\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for path in tqdm(paths[:int(len(paths))]):\n",
    "    X, y = create_dataset_from_json(path, standing=0) # use only winners' moves\n",
    "    if X is not 0:\n",
    "        X_train.append(X)\n",
    "        y_train.append(y)\n",
    "    \n",
    "X_train = np.concatenate(X_train)\n",
    "y_train = np.concatenate(y_train)\n",
    "\n",
    "X_train, unique_index = np.unique(X_train, axis=0, return_index=True)\n",
    "y_train = y_train[unique_index]\n",
    "\n",
    "X_train = np.transpose(X_train, [0, 2, 3, 1])\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.3\n",
    "batch_size = 32\n",
    "val_size = 0.1\n",
    "\n",
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=71)\n",
    "\n",
    "training_generator = GeeseDataGenerator(X_train_, (1-alpha)*y_train_+0.25*alpha, batch_size=batch_size)\n",
    "cos = CosineAnnealingScheduler(10, 2e-3*batch_size/32, 0, verbose=1)\n",
    "\n",
    "model = GeeseNet(input_shape=(7, 11, 17), layers=12, filters=32, weight_decay=1e-7, compile=True)\n",
    "model.fit(training_generator, validation_data=(X_val, y_val), epochs=11, batch_size=batch_size, callbacks=[cos])\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write weight to the head of the python script\n",
    "weight_base64 = base64.b64encode(bz2.compress(pickle.dumps(model.get_weights())))\n",
    "w = \"weight= %s\"%weight_base64\n",
    "%store w >submission.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a submission.py\n",
    "import pickle\n",
    "import bz2\n",
    "import base64\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Conv2D, Activation, Lambda, Add, BatchNormalization, Input\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l1_l2, l2\n",
    "\n",
    "\n",
    "# Neural Network for Hungry Geese\n",
    "def TorusConv2D(x, ch, kernel, padding=\"same\", strides=1, weight_decay=2e-3):\n",
    "    x = Lambda(lambda x: K.tile(x, n=(1,3,3,1)), \n",
    "               output_shape=lambda input_shape: (None, 3*input_shape[1], 3*input_shape[2], input_shape[3]))(x)\n",
    "    \n",
    "    x = Conv2D(ch, kernel, padding=padding, strides=strides,\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    \n",
    "    x = Lambda(lambda x: x[:,int(x.shape[1]/3):2*int(x.shape[1]/3), int(x.shape[2]/3):2*int(x.shape[2]/3),:], \n",
    "               output_shape=lambda input_shape: (None, int(input_shape[1]/3), int(input_shape[2]/3), input_shape[3]))(x)\n",
    "    return x\n",
    "\n",
    "def conv_bn_relu(x0, ch, kernel, padding=\"same\", strides=1, weight_decay=2e-3, add=False):\n",
    "    x = TorusConv2D(x0, ch, kernel, padding=padding, strides=strides,\n",
    "                      weight_decay=weight_decay)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    if add:\n",
    "        x = Add()([x0, x])\n",
    "    return x\n",
    "\n",
    "def GeeseNet(input_shape=(7, 11, 17), layers=12, filters=32, weight_decay=2e-3):\n",
    "    input = Input(input_shape)\n",
    "    x = conv_bn_relu(input, filters, 3)\n",
    "    \n",
    "    for i in range(layers):\n",
    "        x = conv_bn_relu(x, filters, 3, add=True)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    output = Dense(4, activation='softmax', kernel_regularizer=l1_l2(l1=0.0005, l2=0.0005))(x)   \n",
    "    model = Model(input, output)\n",
    "    #model.compile(optimizer=RadaBelief(learning_rate=1e-3, epsilon=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])    \n",
    "    \n",
    "    return model\n",
    "\n",
    "# Input for Neural Network\n",
    "def centerize(b):\n",
    "    dy, dx = np.where(b[0])\n",
    "    centerize_y = (np.arange(0,7)-3+dy[0])%7\n",
    "    centerize_x = (np.arange(0,11)-5+dx[0])%11\n",
    "    \n",
    "    b = b[:, centerize_y,:]\n",
    "    b = b[:, :,centerize_x]\n",
    "    \n",
    "    return b\n",
    "\n",
    "def make_input(obses):\n",
    "    b = np.zeros((17, 7 * 11), dtype=np.float32)\n",
    "    obs = obses[-1]\n",
    "\n",
    "    for p, pos_list in enumerate(obs['geese']):\n",
    "        # head position\n",
    "        for pos in pos_list[:1]:\n",
    "            b[0 + (p - obs['index']) % 4, pos] = 1\n",
    "        # tip position\n",
    "        for pos in pos_list[-1:]:\n",
    "            b[4 + (p - obs['index']) % 4, pos] = 1\n",
    "        # whole position\n",
    "        for pos in pos_list:\n",
    "            b[8 + (p - obs['index']) % 4, pos] = 1\n",
    "            \n",
    "    # previous head position\n",
    "    if len(obses) > 1:\n",
    "        obs_prev = obses[-2]\n",
    "        for p, pos_list in enumerate(obs_prev['geese']):\n",
    "            for pos in pos_list[:1]:\n",
    "                b[12 + (p - obs['index']) % 4, pos] = 1\n",
    "\n",
    "    # food\n",
    "    for pos in obs['food']:\n",
    "        b[16, pos] = 1\n",
    "        \n",
    "    b = b.reshape(-1, 7, 11)\n",
    "    b = centerize(b) # Where to place the head is arbiterary dicision.\n",
    "\n",
    "    return b\n",
    "\n",
    "\n",
    "# Load Keras Model\n",
    "model = GeeseNet(input_shape=(7, 11, 17), layers=12, filters=32, weight_decay=1e-7)\n",
    "model.set_weights(pickle.loads(bz2.decompress(base64.b64decode(weight))))\n",
    "\n",
    "# Main Function of Agent\n",
    "\n",
    "obses = []\n",
    "\n",
    "def agent(obs_dict, config_dict):\n",
    "    obses.append(obs_dict)\n",
    "\n",
    "    X_test = make_input(obses)\n",
    "    X_test = np.transpose(X_test, (1,2,0))\n",
    "    X_test = X_test.reshape(-1,7,11,17) # channel last.\n",
    "    \n",
    "    # avoid suicide\n",
    "    obstacles = X_test[:,:,:,[8,9,10,11,12]].max(axis=3) - X_test[:,:,:,[4,5,6,7]].max(axis=3) # body + opposite_side - my tail\n",
    "    obstacles = np.array([obstacles[0,2,5], obstacles[0,4,5], obstacles[0,3,4], obstacles[0,3,6]])\n",
    "    \n",
    "    y_pred = model.predict(X_test) - obstacles\n",
    "\n",
    "    \n",
    "    \n",
    "    actions = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n",
    "    return actions[np.argmax(y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "env = make(\"hungry_geese\", debug=True)\n",
    "# enemy = '../input/nejugeese/submission_baseline.py'\n",
    "\n",
    "env.reset()\n",
    "env.run(['submission.py','submission.py','submission.py','submission.py'])\n",
    "env.render(mode=\"ipython\", width=600, height=500)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1917445,
     "sourceId": 25401,
     "sourceType": "competition"
    },
    {
     "datasetId": 4538360,
     "sourceId": 7760230,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4538700,
     "sourceId": 7760682,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4542976,
     "sourceId": 7766608,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4554895,
     "sourceId": 7783138,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 57013394,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30068,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
