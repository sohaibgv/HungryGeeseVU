{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create Hungry Geese agent by supervised learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this Notebook is to provide an easy way to participate in this competition, so that many people can enjoy Hungry Geese. Kagglers are familiar with supervised learning and keras is one of the easiest frameworks to use. So, I would like to introduce a simple approach using keras with supervised learning. Of course, if you are aiming for a higher level, you will need reinforcement learning. I'm learning RL through this competition. Let's enjoy Hungry Geese together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on a lot of public notebooks. In particular, the following ones:  \n",
    "Most of technical parts including model architecture, data pipeline etc: https://www.kaggle.com/yuricat/smart-geese-trained-by-reinforcement-learning  \n",
    "Scraping episode files: https://www.kaggle.com/robga/simulations-episode-scraper-match-downloader  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "import bz2\n",
    "import base64\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score, recall_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import (GlobalAveragePooling2D, Dense, Conv2D, Activation, Lambda, Add, \n",
    "                                     BatchNormalization, Input, Lambda)\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l1_l2 ,l1, l2\n",
    "from tensorflow.keras import mixed_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radabelief Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/ben-arnao/Radabelief\n",
    "import tensorflow as tf\n",
    "from tensorflow_addons.utils.types import FloatTensorLike\n",
    "\n",
    "from typing import Union, Callable, Dict\n",
    "from typeguard import typechecked\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
    "class RadaBelief(tf.keras.optimizers.Optimizer):\n",
    "\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "            self,\n",
    "            learning_rate: Union[FloatTensorLike, Callable, Dict] = 1e-4,\n",
    "            beta_1: FloatTensorLike = 0.9,\n",
    "            beta_2: FloatTensorLike = 0.999,\n",
    "            epsilon: FloatTensorLike = 1e-12,\n",
    "            warmup_steps: int = 10000,\n",
    "            name: str = \"Radabelief\",\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(name, **kwargs)\n",
    "\n",
    "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n",
    "        self._set_hyper(\"beta_1\", beta_1)\n",
    "        self._set_hyper(\"beta_2\", beta_2)\n",
    "        self._set_hyper(\"warmup_steps\", warmup_steps)\n",
    "        self.epsilon = epsilon or tf.keras.backend.epsilon()\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"m\")\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"v\")\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        params = self.weights\n",
    "        num_vars = int((len(params) - 1) / 2)\n",
    "        if len(weights) == 3 * num_vars + 1:\n",
    "            weights = weights[: len(params)]\n",
    "        super().set_weights(weights)\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "\n",
    "        # *current* learn rate of optimizer (changed by ReduceLROnPlateau for example)\n",
    "        lr_t = self.lr\n",
    "\n",
    "        # get previous moments\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        v = self.get_slot(var, \"v\")\n",
    "\n",
    "        # static hyperparameters of optimizer\n",
    "        lr_0 = self._get_hyper(\"learning_rate\", var_dtype)\n",
    "        warmup_steps = self._get_hyper(\"warmup_steps\", var_dtype)\n",
    "        beta_1 = self._get_hyper(\"beta_1\", var_dtype)\n",
    "        beta_2 = self._get_hyper(\"beta_2\", var_dtype)\n",
    "\n",
    "        # smaller epsilon == more bias == more like SGD\n",
    "        # larger epsilon == more adaptive, potential for large LR difference between variables\n",
    "        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n",
    "\n",
    "        # current step of optimizer\n",
    "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
    "\n",
    "        # use linearly scaled lr from 0 to initial LR while steps under 'warmup_steps'\n",
    "        lr = tf.where(\n",
    "            local_step <= warmup_steps,\n",
    "            (local_step / warmup_steps) * lr_0,\n",
    "            lr_t,\n",
    "        )\n",
    "\n",
    "        # calculate first moment of gradient (momemtum)\n",
    "        m_t = m.assign(\n",
    "            beta_1 * m + (1.0 - beta_1) * grad,\n",
    "            use_locking=self._use_locking\n",
    "        )\n",
    "\n",
    "        # calculate second moment of gradient (RMSprop)\n",
    "        # use 'tf.square(grad - m_t)' for Adabelief instead of 'tf.square(grad)'\n",
    "        v_t = v.assign(\n",
    "            beta_2 * v + (1.0 - beta_2) * tf.square(grad - m_t),\n",
    "            use_locking=self._use_locking,\n",
    "        )\n",
    "\n",
    "        # correct bias (mostly affects initial steps)\n",
    "        m_corr_t = m_t / (1.0 - tf.pow(beta_1, local_step))\n",
    "        v_corr_t = v_t / (1.0 - tf.pow(beta_2, local_step))\n",
    "\n",
    "        # calculate step\n",
    "        var_t = m_corr_t / (tf.sqrt(v_corr_t) + epsilon_t)\n",
    "\n",
    "        # apply learn rate\n",
    "        var_update = var.assign_sub(lr * var_t,\n",
    "                                    use_locking=self._use_locking)\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        return tf.group(*updates)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "                \"beta_1\": self._serialize_hyperparameter(\"beta_1\"),\n",
    "                \"beta_2\": self._serialize_hyperparameter(\"beta_2\"),\n",
    "                \"epsilon\": self.epsilon,\n",
    "                \"warmup_steps\": self._serialize_hyperparameter(\"warmup_steps\"),\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Anealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "    \"\"\"Cosine annealing scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# besed on https://github.com/yu4u/mixup-generator\n",
    "class GeeseDataGenerator(Sequence):\n",
    "        'Generates data for Keras'\n",
    "        def __init__(self, X_train, y_train, batch_size=128, shuffle=True, sample_weight=None):\n",
    "            'Initialization'\n",
    "            self.batch_size = batch_size\n",
    "            self.X_train = X_train\n",
    "            self.y_train = y_train\n",
    "            self.shuffle = shuffle\n",
    "            self.on_epoch_end()\n",
    "            self.sample_weight = sample_weight\n",
    "    \n",
    "        def __len__(self):\n",
    "            'Denotes the number of batches per epoch'\n",
    "            return int(np.ceil(len(self.X_train) / self.batch_size))\n",
    "    \n",
    "        def __getitem__(self, index):\n",
    "            'Generate one batch of data'\n",
    "            # Generate indexes of the batch\n",
    "            indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "            # Generate data\n",
    "            if self.sample_weight is not None:\n",
    "                X, y, w = self.__data_generation(indexes)\n",
    "                return X, y, w\n",
    "            else:\n",
    "                X, y = self.__data_generation(indexes)\n",
    "                return X, y\n",
    "    \n",
    "        def on_epoch_end(self):\n",
    "            'Updates indexes after each epoch'\n",
    "            self.indexes = np.arange(len(self.X_train))\n",
    "            if self.shuffle == True:\n",
    "                np.random.shuffle(self.indexes)\n",
    "        \n",
    "        def __data_generation(self, batch_ids):\n",
    "            #_, h, w, c = self.X_train.shape\n",
    "        \n",
    "            X = self.X_train[batch_ids]\n",
    "            y = self.y_train[batch_ids]\n",
    "            if self.sample_weight is not None:\n",
    "                w = self.sample_weight[batch_ids]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            series_len = len(X)\n",
    "            \n",
    "            # vertical flip\n",
    "            if np.random.uniform(0,1)>0.5:\n",
    "                X = np.flip(X, axis=2)\n",
    "                y = y[:,[0,1,3,2]]\n",
    "                \n",
    "            # horizontal flip\n",
    "            if np.random.uniform(0,1)>0.5:\n",
    "                X = np.flip(X, axis=1)\n",
    "                y = y[:,[1,0,2,3]] \n",
    "            \n",
    "            if self.sample_weight is not None:\n",
    "                return X, y, w\n",
    "            else:\n",
    "                return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Data from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def create_dataset_from_json(filepath, json_object=None, standing=0):\n",
    "    if json_object is None:\n",
    "        json_open = open(path, 'r')\n",
    "        json_load = json.load(json_open)\n",
    "    else:\n",
    "        json_load = json_object\n",
    "        \n",
    "    try:\n",
    "        winner_index = np.argmax(np.argsort(json_load['rewards']) == 3-standing)\n",
    "\n",
    "        obses = []\n",
    "        X = []\n",
    "        y = []\n",
    "        actions = {'NORTH':0, 'SOUTH':1, 'WEST':2, 'EAST':3}\n",
    "\n",
    "        for i in range(len(json_load['steps'])-1):\n",
    "            if json_load['steps'][i][winner_index]['status'] == 'ACTIVE':\n",
    "                y_= json_load['steps'][i+1][winner_index]['action']\n",
    "                if y_ is not None:\n",
    "                    step = json_load['steps'][i]\n",
    "                    step[winner_index]['observation']['geese'] = step[0]['observation']['geese']\n",
    "                    step[winner_index]['observation']['food'] = step[0]['observation']['food']\n",
    "                    obses.append(step[winner_index]['observation'])\n",
    "                    y.append(actions[y_])        \n",
    "\n",
    "        for j in range(len(obses)):\n",
    "            X_ = make_input(obses[:j+1])\n",
    "            X_ = centerize(X_)\n",
    "            X.append(X_)\n",
    "\n",
    "        X = np.array(X, dtype=np.float32)#[starting_step:]\n",
    "        y = np.array(y, dtype=np.uint8)#[starting_step:]\n",
    "\n",
    "        return X, y\n",
    "    except:\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# based on https://www.kaggle.com/yuricat/smart-geese-trained-by-reinforcement-learning\n",
    "def centerize(b):\n",
    "    dy, dx = np.where(b[0])\n",
    "    centerize_y = (np.arange(0,7)-3+dy[0])%7\n",
    "    centerize_x = (np.arange(0,11)-5+dx[0])%11\n",
    "    \n",
    "    b = b[:, centerize_y,:]\n",
    "    b = b[:, :,centerize_x]\n",
    "    \n",
    "    return b\n",
    "\n",
    "def make_input(obses):\n",
    "    b = np.zeros((17, 7 * 11), dtype=np.float32)\n",
    "    obs = obses[-1]\n",
    "\n",
    "    for p, pos_list in enumerate(obs['geese']):\n",
    "        # head position\n",
    "        for pos in pos_list[:1]:\n",
    "            b[0 + (p - obs['index']) % 4, pos] = 1\n",
    "        # tip position\n",
    "        for pos in pos_list[-1:]:\n",
    "            b[4 + (p - obs['index']) % 4, pos] = 1\n",
    "        # whole position\n",
    "        for pos in pos_list:\n",
    "            b[8 + (p - obs['index']) % 4, pos] = 1\n",
    "            \n",
    "    # previous head position\n",
    "    if len(obses) > 1:\n",
    "        obs_prev = obses[-2]\n",
    "        for p, pos_list in enumerate(obs_prev['geese']):\n",
    "            for pos in pos_list[:1]:\n",
    "                b[12 + (p - obs['index']) % 4, pos] = 1\n",
    "\n",
    "    # food\n",
    "    for pos in obs['food']:\n",
    "        b[16, pos] = 1\n",
    "        \n",
    "    b = b.reshape(-1, 7, 11)\n",
    "    b = centerize(b) # Where to place the head is arbiterary dicision.\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# based on https://www.kaggle.com/yuricat/smart-geese-trained-by-reinforcement-learning\n",
    "def TorusConv2D(x, ch, kernel, padding=\"same\", strides=1, weight_decay=2e-3):\n",
    "    x = Lambda(lambda x: K.tile(x, n=(1,3,3,1)), \n",
    "               output_shape=lambda input_shape: (None, 3*input_shape[1], 3*input_shape[2], input_shape[3]))(x)\n",
    "    \n",
    "    x = Conv2D(ch, kernel, padding=padding, strides=strides,\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    \n",
    "    x = Lambda(lambda x: x[:,int(x.shape[1]/3):2*int(x.shape[1]/3), int(x.shape[2]/3):2*int(x.shape[2]/3),:], \n",
    "               output_shape=lambda input_shape: (None, int(input_shape[1]/3), int(input_shape[2]/3), input_shape[3]))(x)\n",
    "    return x\n",
    "\n",
    "def conv_bn_relu(x0, ch, kernel, padding=\"same\", strides=1, weight_decay=2e-3, add=False):\n",
    "    x = TorusConv2D(x0, ch, kernel, padding=padding, strides=strides,\n",
    "                      weight_decay=weight_decay)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    if add:\n",
    "        x = Add()([x0, x])\n",
    "    return x\n",
    "\n",
    "def GeeseNet(input_shape=(7, 11, 17), layers=12, filters=32, weight_decay=2e-3, compile=True):\n",
    "    input = Input(input_shape)\n",
    "    x = conv_bn_relu(input, filters, 3)\n",
    "    \n",
    "    for i in range(layers):\n",
    "        x = conv_bn_relu(x, filters, 3, add=True)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    output = Dense(4, activation='softmax', kernel_regularizer=l1_l2(l1=0.0005, l2=0.0005))(x)   \n",
    "    model = Model(input, output)\n",
    "    if compile:\n",
    "        model.compile(optimizer=RadaBelief(learning_rate=1e-3, epsilon=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class NBatchLogger(Callback):\n",
    "    \"A Logger that log average performance per `display` steps.\"\n",
    "\n",
    "    def __init__(self, display):\n",
    "        self.step = 0\n",
    "        self.display = display\n",
    "        self.metric_cache = {}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.step += 1\n",
    "        for k in self.params['metrics']:\n",
    "            if k in logs:\n",
    "                self.metric_cache[k] = self.metric_cache.get(k, 0) + logs[k]\n",
    "        if self.step % self.display == 0:\n",
    "            metrics_log = ''\n",
    "            for (k, v) in self.metric_cache.items():\n",
    "                val = v / self.display\n",
    "                if abs(val) > 1e-3:\n",
    "                    metrics_log += ' - %s: %.4f' % (k, val)\n",
    "                else:\n",
    "                    metrics_log += ' - %s: %.4e' % (k, val)\n",
    "            print(f'step: {self.step}/{self.params[\"steps\"]} ... {metrics_log}')       \n",
    "            self.metric_cache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 2070 SUPER, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "# use mixed precision\n",
    "policy = mixed_precision.Policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178a266a56d842239c6aaea42443796b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create data set from json\n",
    "# Only a few episodes are used to train here. Try it with your own json collection.\n",
    "# Replace with other unzipped folders to load other games in and train using those.\n",
    "paths = [path for path in glob(\"./flood_fill_3k_data/results_0_to_3K/*.json\") if 'info' not in path]\n",
    "\n",
    "print(len(paths))\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for path in tqdm(paths[:int(len(paths))]):\n",
    "    X, y = create_dataset_from_json(path, standing=0) # use only winners' moves\n",
    "    if X is not 0:\n",
    "        X_train.append(X)\n",
    "        y_train.append(y)\n",
    "    \n",
    "X_train = np.concatenate(X_train)\n",
    "y_train = np.concatenate(y_train)\n",
    "\n",
    "X_train, unique_index = np.unique(X_train, axis=0, return_index=True)\n",
    "y_train = y_train[unique_index]\n",
    "\n",
    "X_train = np.transpose(X_train, [0, 2, 3, 1])\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((365109, 7, 11, 17), (365109, 4))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'save_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2896\\1444440492.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtraining_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGeeseDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0my_train_\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCosineAnnealingScheduler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2e-3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_history\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGeeseNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m17\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'save_history'"
     ]
    }
   ],
   "source": [
    "alpha=0.3\n",
    "batch_size = 32\n",
    "val_size = 0.1\n",
    "\n",
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=71)\n",
    "\n",
    "training_generator = GeeseDataGenerator(X_train_, (1-alpha)*y_train_+0.25*alpha, batch_size=batch_size)\n",
    "cos = CosineAnnealingScheduler(10, 2e-3*batch_size/32, 0, verbose=1)\n",
    "\n",
    "model = GeeseNet(input_shape=(7, 11, 17), layers=12, filters=32, weight_decay=1e-7, compile=True)\n",
    "history = model.fit(training_generator, validation_data=(X_val, y_val), epochs=11, batch_size=batch_size, callbacks=[cos])\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./imitate-flood-fill.log', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write weight to the head of the python script\n",
    "weight_base64 = base64.b64encode(bz2.compress(pickle.dumps(model.get_weights())))\n",
    "w = \"weight= %s\"%weight_base64\n",
    "%store w >submission.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "timestamps = []\n",
    "accuracies = []\n",
    "losses = []\n",
    "learning_rates = []\n",
    "learning_rate_epochs =[]\n",
    "log_file_path = \"./flood_fill_3k_data/log.txt\"\n",
    "\n",
    "with open(log_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if 'learning rate' in line:\n",
    "            learning_rate_match = re.search(r'Epoch (\\d+).*setting learning rate to (\\d+\\.\\d+)', line)\n",
    "            epoch = int(learning_rate_match.group(1))\n",
    "            learning_rate = float(learning_rate_match.group(2))\n",
    "            \n",
    "            # if learning_rate<4:\n",
    "            learning_rates.append(learning_rate)\n",
    "            learning_rate_epochs.append(epoch)\n",
    "        if '- loss ' in line:\n",
    "            loss_match = re.search(r'loss (\\d+\\.\\d+)', line)\n",
    "            accuracy_match = re.search(r'accuracy (\\d+\\.\\d+)', line)\n",
    "            if loss_match and accuracy_match:\n",
    "                loss = float(loss_match.group(1))\n",
    "                accuracy = float(accuracy_match.group(1))\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "print(losses)\n",
    "plt.figure(figsize=(50, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.figure(figsize=(50, 6))\n",
    "plt.plot(learning_rate_epochs, learning_rates, marker='o', color='b', label='Learning Rate')\n",
    "plt.title('Learning Rate Change (Epochs 0-11)', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Learning Rate', fontsize=14)\n",
    "plt.xticks(learning_rate_epochs, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a submission.py\n",
    "import pickle\n",
    "import bz2\n",
    "import base64\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Conv2D, Activation, Lambda, Add, BatchNormalization, Input\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l1_l2, l2\n",
    "\n",
    "\n",
    "# Neural Network for Hungry Geese\n",
    "def TorusConv2D(x, ch, kernel, padding=\"same\", strides=1, weight_decay=2e-3):\n",
    "    x = Lambda(lambda x: K.tile(x, n=(1,3,3,1)), \n",
    "               output_shape=lambda input_shape: (None, 3*input_shape[1], 3*input_shape[2], input_shape[3]))(x)\n",
    "    \n",
    "    x = Conv2D(ch, kernel, padding=padding, strides=strides,\n",
    "                      kernel_regularizer=l2(weight_decay))(x)\n",
    "    \n",
    "    x = Lambda(lambda x: x[:,int(x.shape[1]/3):2*int(x.shape[1]/3), int(x.shape[2]/3):2*int(x.shape[2]/3),:], \n",
    "               output_shape=lambda input_shape: (None, int(input_shape[1]/3), int(input_shape[2]/3), input_shape[3]))(x)\n",
    "    return x\n",
    "\n",
    "def conv_bn_relu(x0, ch, kernel, padding=\"same\", strides=1, weight_decay=2e-3, add=False):\n",
    "    x = TorusConv2D(x0, ch, kernel, padding=padding, strides=strides,\n",
    "                      weight_decay=weight_decay)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    if add:\n",
    "        x = Add()([x0, x])\n",
    "    return x\n",
    "\n",
    "def GeeseNet(input_shape=(7, 11, 17), layers=12, filters=32, weight_decay=2e-3):\n",
    "    input = Input(input_shape)\n",
    "    x = conv_bn_relu(input, filters, 3)\n",
    "    \n",
    "    for i in range(layers):\n",
    "        x = conv_bn_relu(x, filters, 3, add=True)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    output = Dense(4, activation='softmax', kernel_regularizer=l1_l2(l1=0.0005, l2=0.0005))(x)   \n",
    "    model = Model(input, output)\n",
    "    #model.compile(optimizer=RadaBelief(learning_rate=1e-3, epsilon=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])    \n",
    "    \n",
    "    return model\n",
    "\n",
    "# Input for Neural Network\n",
    "def centerize(b):\n",
    "    dy, dx = np.where(b[0])\n",
    "    centerize_y = (np.arange(0,7)-3+dy[0])%7\n",
    "    centerize_x = (np.arange(0,11)-5+dx[0])%11\n",
    "    \n",
    "    b = b[:, centerize_y,:]\n",
    "    b = b[:, :,centerize_x]\n",
    "    \n",
    "    return b\n",
    "\n",
    "def make_input(obses):\n",
    "    b = np.zeros((17, 7 * 11), dtype=np.float32)\n",
    "    obs = obses[-1]\n",
    "\n",
    "    for p, pos_list in enumerate(obs['geese']):\n",
    "        # head position\n",
    "        for pos in pos_list[:1]:\n",
    "            b[0 + (p - obs['index']) % 4, pos] = 1\n",
    "        # tip position\n",
    "        for pos in pos_list[-1:]:\n",
    "            b[4 + (p - obs['index']) % 4, pos] = 1\n",
    "        # whole position\n",
    "        for pos in pos_list:\n",
    "            b[8 + (p - obs['index']) % 4, pos] = 1\n",
    "            \n",
    "    # previous head position\n",
    "    if len(obses) > 1:\n",
    "        obs_prev = obses[-2]\n",
    "        for p, pos_list in enumerate(obs_prev['geese']):\n",
    "            for pos in pos_list[:1]:\n",
    "                b[12 + (p - obs['index']) % 4, pos] = 1\n",
    "\n",
    "    # food\n",
    "    for pos in obs['food']:\n",
    "        b[16, pos] = 1\n",
    "        \n",
    "    b = b.reshape(-1, 7, 11)\n",
    "    b = centerize(b) # Where to place the head is arbiterary dicision.\n",
    "\n",
    "    return b\n",
    "\n",
    "\n",
    "# Load Keras Model\n",
    "model = GeeseNet(input_shape=(7, 11, 17), layers=12, filters=32, weight_decay=1e-7)\n",
    "model.set_weights(pickle.loads(bz2.decompress(base64.b64decode(weight))))\n",
    "\n",
    "# Main Function of Agent\n",
    "\n",
    "obses = []\n",
    "\n",
    "def agent(obs_dict, config_dict):\n",
    "    obses.append(obs_dict)\n",
    "\n",
    "    X_test = make_input(obses)\n",
    "    X_test = np.transpose(X_test, (1,2,0))\n",
    "    X_test = X_test.reshape(-1,7,11,17) # channel last.\n",
    "    \n",
    "    # avoid suicide\n",
    "    obstacles = X_test[:,:,:,[8,9,10,11,12]].max(axis=3) - X_test[:,:,:,[4,5,6,7]].max(axis=3) # body + opposite_side - my tail\n",
    "    obstacles = np.array([obstacles[0,2,5], obstacles[0,4,5], obstacles[0,3,4], obstacles[0,3,6]])\n",
    "    \n",
    "    y_pred = model.predict(X_test) - obstacles\n",
    "\n",
    "    \n",
    "    \n",
    "    actions = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n",
    "    return actions[np.argmax(y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "env = make(\"hungry_geese\", debug=True)\n",
    "# enemy = '../input/nejugeese/submission_baseline.py'\n",
    "\n",
    "env.reset()\n",
    "env.run(['submission.py','submission.py','submission.py','submission.py'])\n",
    "env.render(mode=\"ipython\", width=600, height=500)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1917445,
     "sourceId": 25401,
     "sourceType": "competition"
    },
    {
     "datasetId": 4538360,
     "sourceId": 7760230,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4538700,
     "sourceId": 7760682,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4542976,
     "sourceId": 7766608,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4554895,
     "sourceId": 7783138,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 57013394,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30068,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "hungry_geese"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
